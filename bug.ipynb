{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dd2dd3-5a3f-4e2d-822d-17586469d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tinygrad in ./.local/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tinygrad numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ae84f3-896d-4e1b-b1c6-a7173bce618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'tinygrad.device' found in sys.modules after import of package 'tinygrad', but prior to execution of 'tinygrad.device'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "  METAL     : \u001b[31mFAIL\u001b[0m /usr/lib/libobjc.dylib: cannot open shared object file: No such file or directory\n",
      "  AMD       : \u001b[31mFAIL\u001b[0m No interface for AMD:0 is available\n",
      "  NV        : \u001b[31mFAIL\u001b[0m rm_control returned 58: Invalid structure parameter\n",
      "* CUDA      : \u001b[32mPASS\u001b[0m\n",
      "                 \u001b[32m+\u001b[0m CUDACompiler (default)\n",
      "                 \u001b[32m+\u001b[0m PTXCompiler (CUDA_PTX=1 to make default)\n",
      "                 \u001b[32m+\u001b[0m NVCCCompiler (CUDA_NVCC=1 to make default)\n",
      "  QCOM      : \u001b[31mFAIL\u001b[0m [Errno 2] No such file or directory: '/dev/kgsl-3d0'\n",
      "  CL        : \u001b[31mFAIL\u001b[0m OpenCL Error -1001: Unknown error\n",
      "  CPU       : \u001b[32mPASS\u001b[0m\n",
      "                 \u001b[32m+\u001b[0m ClangJITCompiler (default)\n",
      "                 \u001b[32m+\u001b[0m CPULLVMCompiler (CPU_LLVM=1 to make default)\n",
      "                 \u001b[33m-\u001b[0m LVPCompiler: libtinymesa not found (MESA_PATH=/usr/lib). See https://github.com/sirhcm/tinymesa (tinymesa-32dc66c, mesa-25.2.4)\n",
      "  DSP       : \u001b[31mFAIL\u001b[0m [Errno 2] No such file or directory: '/dev/ion'\n",
      "  WEBGPU    : \u001b[31mFAIL\u001b[0m dawn library not found. Install it with `sudo curl -L https://github.com/wpmed92/pydawn/releases/download/v0.3.0/libwebgpu_dawn_x86_64.so -o /usr/lib/libwebgpu_dawn.so`\n"
     ]
    }
   ],
   "source": [
    "!python3 -m tinygrad.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdd9c73-d4da-4c98-8976-e8d089886cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEBUG=2\n"
     ]
    }
   ],
   "source": [
    "%env DEBUG=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c76159-01c0-4f69-9a74-faa58dc65e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "scheduled 26 kernels in 55.32 ms\n",
      "\u001b[32m*** CUDA      81\u001b[0m \u001b[33mcopy    2.36M,    CUDA <- NPY    \u001b[0m              arg  2 mem   0.06 GB tm   8494.44us/122528.94ms (      0 GFLOPS    0|0      GB/s) ['conv2d']\n",
      "*** CUDA      82 E\u001b[90mn7\u001b[0m                                            arg  1 mem   0.06 GB tm     66.94us/122529.00ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "\u001b[32m*** CUDA      83\u001b[0m \u001b[33mcopy       24,    CUDA <- NPY    \u001b[0m              arg  2 mem   0.06 GB tm     34.12us/122529.04ms (      0 GFLOPS    0|0      GB/s) \n",
      "*** CUDA      84 E\u001b[90m_\u001b[0m\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                      arg  3 mem   0.06 GB tm      8.54us/122529.05ms (     74 GFLOPS    1|2      GB/s) ['uniform']\n",
      "*** CUDA      85 E\u001b[90m\u001b[0m                                              arg  2 mem   0.06 GB tm      7.23us/122529.05ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      86 E\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                          arg  3 mem   0.06 GB tm      6.05us/122529.06ms (      1 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      87 E\u001b[90mn1\u001b[0m                                            arg  2 mem   0.06 GB tm      7.39us/122529.07ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      88 r\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m5\u001b[0m\u001b[90mn1\u001b[0m                    arg  4 mem   0.16 GB tm    322.21us/122529.39ms (  15711 GFLOPS  315|9541   GB/s) ['uniform', 'relu', 'conv2d']\n",
      "*** CUDA      89 E\u001b[90m_\u001b[0m\u001b[34m200\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.16 GB tm      6.37us/122529.40ms (   1049 GFLOPS   16|28     GB/s) ['uniform']\n",
      "*** CUDA      90 E\u001b[90m\u001b[0m                                              arg  2 mem   0.16 GB tm      5.15us/122529.40ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      91 E\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                          arg  3 mem   0.16 GB tm      6.30us/122529.41ms (      1 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      92 E\u001b[90mn2\u001b[0m                                            arg  2 mem   0.16 GB tm      5.25us/122529.41ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      93 r\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m\u001b[0m                   arg  4 mem   0.18 GB tm   1532.54us/122530.95ms (  27673 GFLOPS   81|16994  GB/s) ['uniform', 'conv2d', 'relu', 'batchnorm', 'max_pool2d']\n",
      "*** CUDA      94 E\u001b[90m_\u001b[0m\u001b[34m192\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.09 GB tm      5.44us/122530.95ms (    887 GFLOPS   14|27     GB/s) ['uniform']\n",
      "*** CUDA      95 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.09 GB tm      6.27us/122530.96ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      96 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.09 GB tm      6.11us/122530.96ms (      3 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      97 E\u001b[90mn4\u001b[0m                                            arg  2 mem   0.09 GB tm      6.66us/122530.97ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      98 r\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                    arg  4 mem   0.13 GB tm   1310.08us/122532.28ms (   7074 GFLOPS   55|6204   GB/s) ['uniform', 'relu', 'conv2d']\n",
      "*** CUDA      99 E\u001b[90m_\u001b[0m\u001b[34m384\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.11 GB tm      5.79us/122532.29ms (   1665 GFLOPS   25|51     GB/s) ['uniform']\n",
      "*** CUDA     100 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.11 GB tm      4.54us/122532.29ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     101 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.11 GB tm      4.16us/122532.30ms (      4 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     102 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.11 GB tm      4.83us/122532.30ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     103 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.11 GB tm      4.03us/122532.30ms (      4 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     104 E\u001b[90mn5\u001b[0m                                            arg  2 mem   0.11 GB tm      4.93us/122532.31ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     105 E\u001b[90mn6\u001b[0m                                            arg  3 mem   0.11 GB tm      5.22us/122532.31ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA     106 r\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                      arg  7 mem   0.11 GB tm \u001b[33m    46.74s \u001b[0m/169270.94ms (      0 GFLOPS    0|1      GB/s) ['reciprocal', 'uniform', 'conv2d', 'relu', 'batchnorm', 'max_pool2d', 'linear', '__sub__', 'abs', 'mean', '__mul__']\n",
      "[46.81282288301736]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from tinygrad import Tensor, TinyJit, nn\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "class Model:\n",
    "  def __init__(self):\n",
    "    self.layers: list[Callable[[Tensor], Tensor]] = [\n",
    "      nn.Conv2d(3, 32, 5), Tensor.relu,\n",
    "      nn.Conv2d(32, 32, 5), Tensor.relu,\n",
    "      nn.BatchNorm2d(32), Tensor.max_pool2d,\n",
    "      nn.Conv2d(32, 64, 3), Tensor.relu,\n",
    "      nn.Conv2d(64, 64, 3), Tensor.relu,\n",
    "      nn.BatchNorm2d(64), Tensor.max_pool2d,\n",
    "      lambda x: x.mean(axis=(2,3)), nn.Linear(64, 1)]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # X = np.random.randn(16, 3, 512, 512).astype(np.float32)\n",
    "  # Y = np.random.randn(16, 1).astype(np.float32)\n",
    "  X, Y = np.load(\"data/X.npy\"), np.load(\"data/Y.npy\")\n",
    "  permutation = [0, 1, 5, 14, 13, 11, 8, 9, 2, 15, 4, 7, 10, 12, 3, 6]\n",
    "  X_train, X_val = X[permutation[:10]], X[permutation[10:13]]\n",
    "  Y_train, Y_val = Y[permutation[:10]], Y[permutation[10:13]]\n",
    "  X_train, Y_train, X_val, Y_val = map(Tensor, [X_train, Y_train, X_val, Y_val])\n",
    "\n",
    "  model = Model()\n",
    "  opt = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "  \n",
    "  @TinyJit\n",
    "  def get_val_acc() -> Tensor: return ((model(X_val).squeeze()-Y_val)*Y_val.reciprocal()).abs().mean()*100\n",
    "  print(\"starting\")\n",
    "  print(timeit.repeat(get_val_acc, repeat=1, number=1))\n",
    "  print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c96b3-e12e-4a8d-9546-e2fa87ac223e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
