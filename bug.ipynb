{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dd2dd3-5a3f-4e2d-822d-17586469d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tinygrad in ./.local/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tinygrad numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ae84f3-896d-4e1b-b1c6-a7173bce618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'tinygrad.device' found in sys.modules after import of package 'tinygrad', but prior to execution of 'tinygrad.device'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "  METAL     : \u001b[31mFAIL\u001b[0m /usr/lib/libobjc.dylib: cannot open shared object file: No such file or directory\n",
      "  AMD       : \u001b[31mFAIL\u001b[0m No interface for AMD:0 is available\n",
      "  NV        : \u001b[31mFAIL\u001b[0m rm_control returned 58: Invalid structure parameter\n",
      "* CUDA      : \u001b[32mPASS\u001b[0m\n",
      "                 \u001b[32m+\u001b[0m CUDACompiler (default)\n",
      "                 \u001b[32m+\u001b[0m PTXCompiler (CUDA_PTX=1 to make default)\n",
      "                 \u001b[32m+\u001b[0m NVCCCompiler (CUDA_NVCC=1 to make default)\n",
      "  QCOM      : \u001b[31mFAIL\u001b[0m [Errno 2] No such file or directory: '/dev/kgsl-3d0'\n",
      "  CL        : \u001b[31mFAIL\u001b[0m OpenCL Error -1001: Unknown error\n",
      "  CPU       : \u001b[32mPASS\u001b[0m\n",
      "                 \u001b[32m+\u001b[0m ClangJITCompiler (default)\n",
      "                 \u001b[32m+\u001b[0m CPULLVMCompiler (CPU_LLVM=1 to make default)\n",
      "                 \u001b[33m-\u001b[0m LVPCompiler: libtinymesa not found (MESA_PATH=/usr/lib). See https://github.com/sirhcm/tinymesa (tinymesa-32dc66c, mesa-25.2.4)\n",
      "  DSP       : \u001b[31mFAIL\u001b[0m [Errno 2] No such file or directory: '/dev/ion'\n",
      "  WEBGPU    : \u001b[31mFAIL\u001b[0m dawn library not found. Install it with `sudo curl -L https://github.com/wpmed92/pydawn/releases/download/v0.3.0/libwebgpu_dawn_x86_64.so -o /usr/lib/libwebgpu_dawn.so`\n"
     ]
    }
   ],
   "source": [
    "!python3 -m tinygrad.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdd9c73-d4da-4c98-8976-e8d089886cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEBUG=2\n"
     ]
    }
   ],
   "source": [
    "%env DEBUG=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c76159-01c0-4f69-9a74-faa58dc65e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "scheduled 26 kernels in 55.87 ms\n",
      "\u001b[32m*** CUDA      28\u001b[0m \u001b[33mcopy   12.58M,    CUDA <- NPY    \u001b[0m              arg  2 mem   0.03 GB tm   1018.58us/ 46814.50ms (      0 GFLOPS   12|12     GB/s) \n",
      "\u001b[32m*** CUDA      29\u001b[0m E\u001b[90mn7\u001b[0m                                            arg  1 mem   0.01 GB tm     18.56us/ 46814.52ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "\u001b[32m*** CUDA      30\u001b[0m \u001b[33mcopy      128,    CUDA <- NPY    \u001b[0m              arg  2 mem   0.01 GB tm     34.78us/ 46814.55ms (      0 GFLOPS    0|0      GB/s) \n",
      "*** CUDA      31 E\u001b[90m_\u001b[0m\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                      arg  3 mem   0.01 GB tm      7.62us/ 46814.56ms (     83 GFLOPS    1|3      GB/s) ['uniform']\n",
      "*** CUDA      32 E\u001b[90m\u001b[0m                                              arg  2 mem   0.01 GB tm      6.50us/ 46814.57ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      33 E\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                          arg  3 mem   0.01 GB tm      6.85us/ 46814.57ms (      1 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      34 E\u001b[90mn1\u001b[0m                                            arg  2 mem   0.01 GB tm      5.57us/ 46814.58ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      35 r\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m5\u001b[0m\u001b[90m\u001b[0m                      arg  4 mem   0.11 GB tm    324.22us/ 46814.90ms (  15613 GFLOPS  344|9481   GB/s) ['uniform', 'relu', 'conv2d']\n",
      "*** CUDA      36 E\u001b[90m_\u001b[0m\u001b[34m200\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.11 GB tm      7.71us/ 46814.91ms (    866 GFLOPS   13|23     GB/s) ['uniform']\n",
      "*** CUDA      37 E\u001b[90m\u001b[0m                                              arg  2 mem   0.11 GB tm      4.06us/ 46814.91ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      38 E\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                          arg  3 mem   0.11 GB tm      5.02us/ 46814.92ms (      2 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      39 E\u001b[90mn2\u001b[0m                                            arg  2 mem   0.11 GB tm      5.18us/ 46814.92ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      40 r\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m\u001b[0m                   arg  4 mem   0.14 GB tm   1543.58us/ 46816.47ms (  27475 GFLOPS   80|16872  GB/s) ['uniform', 'conv2d', 'relu', 'batchnorm', 'max_pool2d']\n",
      "*** CUDA      41 E\u001b[90m_\u001b[0m\u001b[34m192\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.04 GB tm      5.79us/ 46816.47ms (    833 GFLOPS   13|25     GB/s) ['uniform']\n",
      "*** CUDA      42 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.04 GB tm      5.76us/ 46816.48ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      43 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.04 GB tm      5.92us/ 46816.48ms (      3 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      44 E\u001b[90mn4\u001b[0m                                            arg  2 mem   0.04 GB tm      4.51us/ 46816.49ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      45 r\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                    arg  4 mem   0.09 GB tm   1300.32us/ 46817.79ms (   7127 GFLOPS   56|6251   GB/s) ['uniform', 'relu', 'conv2d']\n",
      "*** CUDA      46 E\u001b[90m_\u001b[0m\u001b[34m384\u001b[0m\u001b[90m_\u001b[0m\u001b[36m32\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m\u001b[0m                                     arg  3 mem   0.06 GB tm      5.60us/ 46817.80ms (   1723 GFLOPS   26|53     GB/s) ['uniform']\n",
      "*** CUDA      47 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.06 GB tm      4.61us/ 46817.80ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      48 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.06 GB tm      3.74us/ 46817.80ms (      4 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      49 E\u001b[90mn3\u001b[0m                                            arg  2 mem   0.06 GB tm      4.22us/ 46817.81ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      50 E\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                         arg  3 mem   0.06 GB tm      3.71us/ 46817.81ms (      4 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      51 E\u001b[90mn5\u001b[0m                                            arg  2 mem   0.06 GB tm      5.86us/ 46817.82ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      52 E\u001b[90mn6\u001b[0m                                            arg  3 mem   0.06 GB tm      4.48us/ 46817.82ms (      0 GFLOPS    0|0      GB/s) ['uniform']\n",
      "*** CUDA      53 r\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                      arg  7 mem   0.06 GB tm \u001b[33m    46.80s \u001b[0m/ 93619.22ms (      0 GFLOPS    0|1      GB/s) ['reciprocal', 'uniform', 'conv2d', 'relu', 'batchnorm', 'max_pool2d', 'linear', '__sub__', 'abs', 'mean', '__mul__']\n",
      "[46.868936092127115]\n",
      "scheduled 4 kernels in 64.65 ms\n",
      "\u001b[32m*** CUDA      54\u001b[0m r\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[34m127\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m5\u001b[0m\u001b[90mn1\u001b[0m                    arg  4 mem   0.11 GB tm    340.48us/ 93619.56ms (  14868 GFLOPS  328|9029   GB/s) ['relu', 'conv2d']\n",
      "*** CUDA      55 r\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[34m63\u001b[0m\u001b[90m_\u001b[0m\u001b[36m8\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[36m4\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[31m5\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m_\u001b[0m\u001b[35m2\u001b[0m\u001b[90m\u001b[0m                   arg  4 mem   0.14 GB tm   1575.84us/ 93621.13ms (  26913 GFLOPS   78|16527  GB/s) ['conv2d', 'relu', 'batchnorm', 'max_pool2d']\n",
      "*** CUDA      56 r\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[34m125\u001b[0m\u001b[90m_\u001b[0m\u001b[36m16\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[36m2\u001b[0m\u001b[90m_\u001b[0m\u001b[33m3\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m_\u001b[0m\u001b[31m32\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                    arg  4 mem   0.09 GB tm   1302.56us/ 93622.44ms (   7115 GFLOPS   56|6240   GB/s) ['relu', 'conv2d']\n",
      "*** CUDA      57 r\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m3\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m2\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m124\u001b[0m\u001b[90m_\u001b[0m\u001b[31m64\u001b[0m\u001b[90m_\u001b[0m\u001b[35m3\u001b[0m\u001b[90m\u001b[0m                      arg  7 mem   0.06 GB tm \u001b[33m    46.80s \u001b[0m/140425.09ms (      0 GFLOPS    0|1      GB/s) ['reciprocal', 'conv2d', 'relu', 'batchnorm', 'max_pool2d', 'linear', '__sub__', 'abs', 'mean', '__mul__']\n",
      "JIT captured 4 kernels with 0 inputs\n",
      "JIT memory reduced from 171.48 MB -> 123.49 MB, 3 -> 1 bufs\n",
      "[46.926465591881424]\n",
      "JIT GRAPHing batch with 4 kernels on device <tinygrad.runtime.ops_cuda.CUDADevice object at 0x7128acd719f0>\n",
      "\u001b[35m*** CUDA      58\u001b[0m \u001b[36m<batched 4>\u001b[0m                                    arg  0 mem   0.14 GB tm \u001b[33m    46.81s \u001b[0m/187230.43ms (      2 GFLOPS    0|2      GB/s) \n",
      "[46.80570558970794]\n",
      "\u001b[35m*** CUDA      59\u001b[0m \u001b[36m<batched 4>\u001b[0m                                    arg  0 mem   0.14 GB tm \u001b[33m    46.80s \u001b[0m/234033.76ms (      2 GFLOPS    0|2      GB/s) \n",
      "[46.80340132815763]\n",
      "\u001b[35m*** CUDA      60\u001b[0m \u001b[36m<batched 4>\u001b[0m                                    arg  0 mem   0.14 GB tm \u001b[33m    46.80s \u001b[0m/280835.17ms (      2 GFLOPS    0|2      GB/s) \n",
      "[46.80147055210546]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from tinygrad import Tensor, TinyJit, nn\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "class Model:\n",
    "  def __init__(self):\n",
    "    self.layers: list[Callable[[Tensor], Tensor]] = [\n",
    "      nn.Conv2d(3, 32, 5), Tensor.relu,\n",
    "      nn.Conv2d(32, 32, 5), Tensor.relu,\n",
    "      nn.BatchNorm2d(32), Tensor.max_pool2d,\n",
    "      nn.Conv2d(32, 64, 3), Tensor.relu,\n",
    "      nn.Conv2d(64, 64, 3), Tensor.relu,\n",
    "      nn.BatchNorm2d(64), Tensor.max_pool2d,\n",
    "      lambda x: x.mean(axis=(2,3)), nn.Linear(64, 1)]\n",
    "\n",
    "  def __call__(self, x:Tensor) -> Tensor: return x.sequential(self.layers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # X = np.random.randn(16, 3, 512, 512).astype(np.float32)\n",
    "  # Y = np.random.randn(16, 1).astype(np.float32)\n",
    "\n",
    "  # shortening tensor for speed; doesn't affect kernels\n",
    "  X, Y = Tensor(np.load(\"data/X.npy\"))[:3], Tensor(np.load(\"data/Y.npy\"))[:3]\n",
    "\n",
    "  model = Model()\n",
    "  opt = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "  \n",
    "  @TinyJit\n",
    "  def mae() -> Tensor: return ((model(X).squeeze()-Y)*Y.reciprocal()).abs().mean()*100\n",
    "  print(\"starting\")\n",
    "  print(timeit.repeat(mae, repeat=1, number=1))\n",
    "  print(timeit.repeat(mae, repeat=1, number=1))\n",
    "  print(timeit.repeat(mae, repeat=1, number=1))\n",
    "  print(timeit.repeat(mae, repeat=1, number=1))\n",
    "  print(timeit.repeat(mae, repeat=1, number=1))\n",
    "  print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c96b3-e12e-4a8d-9546-e2fa87ac223e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
